{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "**Install Hugging Face**\n",
    "\n",
    "[transformers](https://github.com/huggingface/transformers) package from Hugging Face -- pytorch interface for working with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#-----------------------------------------------------#\n",
    "from transformers import AutoTokenizer\n",
    "#-----------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# universal (for all BERT models)\n",
    "summer2018 = \"C:/Users/stuar/Desktop/NLP/datasets/Summer2018-smoke-alcohol-last-removed/\"\n",
    "\n",
    "# universal (for all BERT models)\n",
    "text_dataset_folder = \"C:/Users/stuar/Desktop/NLP/models/BERT/text_datasets_for_BERT/\"\n",
    "\n",
    "# specific (just for BioELECTRA)\n",
    "dataloader_folder = \"C:/Users/stuar/Desktop/NLP/models/BioELECTRA/dataloaders_for_BioELECTRA/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**binary splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_data_split(text_dataset, binary_label, seed_val): # using test_size of 20% atm\n",
    "    \n",
    "    train_dataset, test_dataset, train_labels, test_labels = train_test_split(text_dataset, binary_labels, \n",
    "                                                        test_size=0.20, random_state=seed_val) # was 808\n",
    "    train_dataset = np.array(train_dataset)\n",
    "    test_dataset = np.array(test_dataset)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_labels = np.array(test_labels)\n",
    "    \n",
    "    return train_dataset, test_dataset, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BioELECTRA tokenizer...\n",
      "Done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading BioELECTRA tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kamalkraj/bioelectra-base-discriminator-pubmed\")\n",
    "print(\"Done\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_documents(text_dataset):\n",
    "\n",
    "    \n",
    "    #----------------------------------------------------------------#\n",
    "    input_ids = [] # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    lengths = [] # Record the length of each sequence (after truncating to 512).\n",
    "    print('Tokenizing comments...')\n",
    "    \n",
    "    for document in text_dataset: # for every document\n",
    "\n",
    "        if ((len(input_ids) % 500) == 0): # Report progress.\n",
    "            print('  Read {:,} comments.'.format(len(input_ids)))\n",
    "\n",
    "        # `encode` will:\n",
    "        #   (1) Tokenize the document.\n",
    "        #   (2) Prepend the `[CLS]` token to the start/Append the `[SEP]` token to the end.\n",
    "        #   (3) Map tokens to their IDs.\n",
    "        encoded_doc = tokenizer.encode(\n",
    "                            document,                  # Document to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            #max_length = 512,         # Truncate all sentences.                        \n",
    "                            #return_tensors = 'pt',    # Return pytorch tensors.\n",
    "                        )\n",
    "\n",
    "        input_ids.append(encoded_doc) # Add the encoded sentence to the list.\n",
    "        lengths.append(len(encoded_doc)) # Record the truncated length.\n",
    "\n",
    "    print('DONE.')\n",
    "    print('{:>10,} comments tokenized'.format(len(input_ids)), \"\\n\")\n",
    "    #----------------------------------------------------------------#\n",
    "    \n",
    "    \n",
    "    #----------------------------------------------------------------#\n",
    "    print('Min length: {:,} tokens'.format(min(lengths)))\n",
    "    print('Max length: {:,} tokens'.format(max(lengths)))\n",
    "    print('Median length: {:,} tokens'.format(int(np.median(lengths))))\n",
    "    print('Mean length: {:,} tokens'.format(np.sum(lengths)//len(lengths)))\n",
    "    #----------------------------------------------------------------#\n",
    "    \n",
    "    \n",
    "    #----------------------------------------------------------------#\n",
    "    num_over = 0\n",
    "    for length in lengths:\n",
    "        if length <= 512:\n",
    "            num_over += 1\n",
    "    print('{:,} of {:,} documents ({:.2%}) in this dataset are less than or equal to 512 tokens.'\n",
    "          .format(num_over, len(lengths), float(num_over) / float(len(lengths))))\n",
    "    \n",
    "    num_over = 0\n",
    "    for length in lengths:\n",
    "        if length <= 448:\n",
    "            num_over += 1\n",
    "    print('{:,} of {:,} documents ({:.2%}) in this dataset are less than or equal to 448 tokens.'\n",
    "          .format(num_over, len(lengths), float(num_over) / float(len(lengths))))\n",
    "    \n",
    "    num_over = 0\n",
    "    for length in lengths:\n",
    "        if length <= 384:\n",
    "            num_over += 1\n",
    "    print('{:,} of {:,} documents ({:.2%}) in this dataset are less than or equal to 384 tokens.'\n",
    "          .format(num_over, len(lengths), float(num_over) / float(len(lengths))))\n",
    "    \n",
    "    num_over = 0\n",
    "    for length in lengths:\n",
    "        if length <= 320:\n",
    "            num_over += 1\n",
    "    print('{:,} of {:,} documents ({:.2%}) in this dataset are less than or equal to 320 tokens.'\n",
    "          .format(num_over, len(lengths), float(num_over) / float(len(lengths))))\n",
    "    \n",
    "    num_over = 0\n",
    "    for length in lengths:\n",
    "        if length <= 256:\n",
    "            num_over += 1\n",
    "    print('{:,} of {:,} documents ({:.2%}) in this dataset are less than or equal to 256 tokens.'\n",
    "          .format(num_over, len(lengths), float(num_over) / float(len(lengths))))\n",
    "    \n",
    "    num_over = 0\n",
    "    for length in lengths:\n",
    "        if length <= 192:\n",
    "            num_over += 1\n",
    "    print('{:,} of {:,} documents ({:.2%}) in this dataset are less than or equal to 192 tokens.'\n",
    "          .format(num_over, len(lengths), float(num_over) / float(len(lengths))))\n",
    "    \n",
    "    num_over = 0\n",
    "    for length in lengths:\n",
    "        if length <= 64:\n",
    "            num_over += 1\n",
    "    print('{:,} of {:,} documents ({:.2%}) in this dataset are less than or equal to 64 tokens.'\n",
    "          .format(num_over, len(lengths), float(num_over) / float(len(lengths))))\n",
    "    #----------------------------------------------------------------#\n",
    "    \n",
    "    \n",
    "    return input_ids, lengths # lengths for potential data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(input_ids, MAX_LEN): # Set the required sequence length.\n",
    "    \n",
    "    print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "    print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "    \n",
    "    input_ids = sequence.pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", # Pad our input tokens with value 0.\n",
    "                              value=0, truncating=\"post\", padding=\"post\")\n",
    "    # truncating=\"post\" --> remove values from sequences longer than maxlen at the END of sequences\n",
    "    # padding=\"post\" --> pad BEFORE each seqeunce\n",
    "    print('\\nDone.')\n",
    "    \n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**attention masks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_attention_masks(input_ids):\n",
    "    attention_masks = [] # Create attention masks\n",
    "    \n",
    "    for encoded_doc in input_ids: # For each encoded document\n",
    "        # Create the attention mask.\n",
    "        #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "        #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "        att_mask = [int(token_id > 0) for token_id in encoded_doc]\n",
    "        attention_masks.append(att_mask) # Store the attention mask for this sentence.\n",
    "        \n",
    "    return attention_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader Creation\n",
    "- **just change the 'name' of the text dataset and binary labels accordingly and run the cells**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2220"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"JessAble_drug_35b165a\"  \n",
    "f = open(f\"{text_dataset_folder}{name}\", \"rb\")\n",
    "text_dataset = pickle.load(f)\n",
    "f.close()\n",
    "len(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2220"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"JessAble_labels_drug\"\n",
    "f = open(f\"{text_dataset_folder}{name}\", \"rb\")\n",
    "binary_labels = pickle.load(f)\n",
    "f.close()\n",
    "len(binary_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train & validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE ACCORDINGLY\n",
    "#--------------------------#\n",
    "SEED = 2001 # set splitting\n",
    "#--------------------------#\n",
    "BATCH_SIZE = 32 \n",
    "MAX_LEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset, train_labels, test_labels = binary_data_split(text_dataset, binary_labels, SEED) \n",
    "list(train_labels).count(1), list(test_labels).count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING PREPROCESSING COMMENCING...\n",
      "-------------------------------------\n",
      "Data split for binary classification.\n",
      "-------------------------------------\n",
      "Tokenizing comments...\n",
      "  Read 0 comments.\n",
      "  Read 500 comments.\n",
      "  Read 1,000 comments.\n",
      "  Read 1,500 comments.\n",
      "DONE.\n",
      "     1,776 comments tokenized \n",
      "\n",
      "Min length: 25 tokens\n",
      "Max length: 91 tokens\n",
      "Median length: 43 tokens\n",
      "Mean length: 44 tokens\n",
      "1,776 of 1,776 documents (100.00%) in this dataset are less than or equal to 512 tokens.\n",
      "1,776 of 1,776 documents (100.00%) in this dataset are less than or equal to 448 tokens.\n",
      "1,776 of 1,776 documents (100.00%) in this dataset are less than or equal to 384 tokens.\n",
      "1,776 of 1,776 documents (100.00%) in this dataset are less than or equal to 320 tokens.\n",
      "1,776 of 1,776 documents (100.00%) in this dataset are less than or equal to 256 tokens.\n",
      "1,776 of 1,776 documents (100.00%) in this dataset are less than or equal to 192 tokens.\n",
      "1,766 of 1,776 documents (99.44%) in this dataset are less than or equal to 64 tokens.\n",
      "-------------------------------------\n",
      "\n",
      "Padding/truncating all sentences to 64 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n",
      "-------------------------------------\n",
      "Attention masks created.\n",
      "-------------------------------------\n",
      "Validation data created.\n",
      "-------------------------------------\n",
      "Train & Validation data converted to PyTorch tensors.\n",
      "-------------------------------------\n",
      "DataLoaders created.\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING PREPROCESSING COMMENCING...\")\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "print(\"-------------------------------------\")\n",
    "train_dataset, test_dataset, train_labels, test_labels = binary_data_split(text_dataset, binary_labels, SEED) # was 808\n",
    "print(\"Data split for binary classification.\")\n",
    "#------------------------------------------------------------#\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "print(\"-------------------------------------\")\n",
    "input_ids, lengths = tokenize_documents(train_dataset)\n",
    "#------------------------------------------------------------#\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "print(\"-------------------------------------\")\n",
    "input_ids = pad_sequences(input_ids, MAX_LEN)\n",
    "#------------------------------------------------------------#\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "print(\"-------------------------------------\")\n",
    "attention_masks = add_attention_masks(input_ids)\n",
    "print(\"Attention masks created.\")\n",
    "#------------------------------------------------------------#\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "print(\"-------------------------------------\")\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, train_labels,\n",
    "                                                       random_state=SEED, test_size=0.1) # was 2021\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, train_labels, \n",
    "                                                       random_state=SEED, test_size=0.1) # was 2021\n",
    "print(\"Validation data created.\")\n",
    "#------------------------------------------------------------#\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "print(\"-------------------------------------\")\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.as_tensor(train_labels, dtype=torch.long)\n",
    "validation_labels = torch.as_tensor(validation_labels, dtype=torch.long)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "print(\"Train & Validation data converted to PyTorch tensors.\")\n",
    "#------------------------------------------------------------#\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "print(\"-------------------------------------\")\n",
    "batch_size = BATCH_SIZE # The DataLoader needs to know our batch size for training\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "print(\"DataLoaders created.\")\n",
    "print(\"DONE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE ACCORDINGLY\n",
    "#---------------------------#\n",
    "SUBSTANCE_FOLDER = \"drug\"     \n",
    "#---------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f\"{dataloader_folder}{SUBSTANCE_FOLDER}/{SEED}/train_DL_JA_35b165a_32_64\", \"wb\")\n",
    "pickle.dump(train_dataloader, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f\"{dataloader_folder}{SUBSTANCE_FOLDER}/{SEED}/valid_DL_JA_35b165a_32_64\", \"wb\")\n",
    "pickle.dump(validation_dataloader, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing comments...\n",
      "  Read 0 comments.\n",
      "DONE.\n",
      "       444 comments tokenized \n",
      "\n",
      "Min length: 29 tokens\n",
      "Max length: 91 tokens\n",
      "Median length: 43 tokens\n",
      "Mean length: 44 tokens\n",
      "444 of 444 documents (100.00%) in this dataset are less than or equal to 512 tokens.\n",
      "444 of 444 documents (100.00%) in this dataset are less than or equal to 448 tokens.\n",
      "444 of 444 documents (100.00%) in this dataset are less than or equal to 384 tokens.\n",
      "444 of 444 documents (100.00%) in this dataset are less than or equal to 320 tokens.\n",
      "444 of 444 documents (100.00%) in this dataset are less than or equal to 256 tokens.\n",
      "444 of 444 documents (100.00%) in this dataset are less than or equal to 192 tokens.\n",
      "442 of 444 documents (99.55%) in this dataset are less than or equal to 64 tokens.\n",
      "\n",
      "Padding/truncating all sentences to 64 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "test_input_ids, lengths = tokenize_documents(test_dataset)\n",
    "#------------------------------------------------------------#\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "test_input_ids = pad_sequences(test_input_ids, MAX_LEN)\n",
    "#------------------------------------------------------------#\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "test_attention_masks = add_attention_masks(test_input_ids)\n",
    "#------------------------------------------------------------#\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "test_labels = torch.as_tensor(test_labels, dtype=torch.long)\n",
    "test_masks = torch.tensor(test_attention_masks)\n",
    "#------------------------------------------------------------#\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "batch_size = BATCH_SIZE # Set the batch size. \n",
    "\n",
    "# Create the DataLoader.\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1598, 64]), torch.Size([178, 64]), torch.Size([444, 64]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ISSUE FIXED\n",
    "train_masks.shape, validation_masks.shape, test_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f\"{dataloader_folder}{SUBSTANCE_FOLDER}/{SEED}/test_DL_JA_35b165a_32_64\", \"wb\")\n",
    "pickle.dump(test_dataloader, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
