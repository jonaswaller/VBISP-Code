{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_labels(word_labels): \n",
    "    '''\n",
    "    - maps word-based labels to binary labels\n",
    "    '''\n",
    "  \n",
    "    smoker_binary = []; drinker_binary = []; duser_binary = []\n",
    "    \n",
    "    # -------------------------------------------------------- #\n",
    "    for label in word_labels:\n",
    "        if (\"Smoker\" in label) and (\"UnkSmoker\" not in label) and (\"NevSmoker\" not in label):\n",
    "            smoker_binary.append(1)\n",
    "        elif (\"NevSmoker\" in label) or (\"UnkSmoker\" in label):\n",
    "            smoker_binary.append(0)\n",
    "        else:\n",
    "            smoker_binary.append(2)\n",
    "\n",
    "    print(f\"{smoker_binary.count(1)} positive labels\")\n",
    "    print(f\"{smoker_binary.count(0)} negative labels\")\n",
    "    print(f\"Do other labels exist? : {2 in smoker_binary} \\n\")\n",
    "    # -------------------------------------------------------- #\n",
    "    \n",
    "    # -------------------------------------------------------- #\n",
    "    for label in word_labels:\n",
    "        if (\"Drinker\" in label) and (\"UnkDrinker\" not in label) and (\"NevDrinker\" not in label):\n",
    "            drinker_binary.append(1)\n",
    "        elif (\"NevDrinker\" in label) or (\"UnkDrinker\" in label):\n",
    "            drinker_binary.append(0)\n",
    "        else:\n",
    "            drinker_binary.append(2)\n",
    "\n",
    "    print(f\"{drinker_binary.count(1)} positive labels\")\n",
    "    print(f\"{drinker_binary.count(0)} negative labels\")\n",
    "    print(f\"Do other labels exist? : {2 in drinker_binary} \\n\")\n",
    "    # -------------------------------------------------------- #\n",
    "    \n",
    "    # -------------------------------------------------------- #\n",
    "    for label in word_labels:\n",
    "        if (\"DUser\" in label) and (\"UnkDUser\" not in label) and (\"NevDUser\" not in label):\n",
    "            duser_binary.append(1)\n",
    "        elif (\"NevDUser\" in label) or (\"UnkDUser\" in label):\n",
    "            duser_binary.append(0)\n",
    "        else:\n",
    "            duser_binary.append(2)\n",
    "    \n",
    "    print(f\"{duser_binary.count(1)} positive labels\")\n",
    "    print(f\"{duser_binary.count(0)} negative labels\")\n",
    "    print(f\"Do other labels exist? : {2 in duser_binary}\")\n",
    "    # -------------------------------------------------------- #\n",
    "        \n",
    "    return smoker_binary, drinker_binary, duser_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(text): \n",
    "  \t'''\n",
    "    - cleans an individual note\n",
    "    - many methods were tested: removing rare words, \n",
    "      stop words, numbers, etc.\n",
    "    - what lies below worked the best\n",
    "    '''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    #----------------------------------------------------------------------#\n",
    "    # removing meaningless text\n",
    "    #text = text[250:] # removing first 250 characters\n",
    "    #text = text[:-250] # removing last 250 characters\n",
    "    #----------------------------------------------------------------------#\n",
    "    \n",
    "    #----------------------------------------------------------------------#\n",
    "    # removing 3-peated letters\n",
    "    for token in text:\n",
    "        if token*3 in text:\n",
    "            text = text.replace(token*3, \"\")\n",
    "    #----------------------------------------------------------------------#\n",
    "    \n",
    "    #----------------------------------------------------------------------#\n",
    "    # tokenizing each word\n",
    "    text = str(word_tokenize(text))\n",
    "    #----------------------------------------------------------------------#\n",
    "    \n",
    "    #----------------------------------------------------------------------#\n",
    "    # replaces slashes around the \\\\n\\\\ with \" \"\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)  \n",
    "\n",
    "    #text = re.sub(r\"[0-9]\", \" \", text) # removes all numbers\n",
    "\n",
    "    # removing common punctuation\n",
    "    text = re.sub(r\"\\,\", \" \", text)                                                  \n",
    "    text = re.sub(r\"\\'\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"\\:\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "\n",
    "    # removing uncommon punctuation\n",
    "    text = re.sub(r\"\\;\", \" \", text)\n",
    "    text = re.sub(r\"\\!\", \" \", text)                                                  \n",
    "    text = re.sub(r\"\\@\", \" \", text)\n",
    "    text = re.sub(r\"\\#\", \" \", text)\n",
    "    text = re.sub(r\"\\$\", \" \", text)\n",
    "    text = re.sub(r\"\\%\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\&\", \" \", text)\n",
    "    text = re.sub(r\"\\*\", \" \", text)\n",
    "    text = re.sub(r\"\\(\", \" \", text)\n",
    "    text = re.sub(r\"\\)\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"\\[\", \" \", text)\n",
    "    text = re.sub(r\"\\]\", \" \", text)\n",
    "    text = re.sub(r\"\\<\", \" \", text)\n",
    "    text = re.sub(r\"\\>\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\" n \", \" \", text)\n",
    "    text = re.sub(r\" b \", \" \", text)\n",
    "    text = re.sub(r\" p \", \" \", text)\n",
    "    text = re.sub(r\" br \", \" \", text)\n",
    "    text = re.sub(r\" em \", \" \", text)\n",
    "    #text = re.sub(r\" mg \", \" \", text)\n",
    "\n",
    "    text = re.sub(r\" - \", \" \", text) \n",
    "    text = re.sub(r\"--\", \" \", text) # removing --s\n",
    "\n",
    "    # 2 or more non-whitespace characters replaced by a space\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text) # makes sense for this to be at end   \n",
    "    \n",
    "    # replaces the dashes b/t tokens \"a-a --> \"aa\" with \"\"\n",
    "    #text = re.sub(\"-([a-zA-Z]+)\", r\"\\1\", text) \n",
    "    #----------------------------------------------------------------------#\n",
    "    \n",
    "    #----------------------------------------------------------------------#\n",
    "    # remove repeated sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = list(dict.fromkeys(sentences))\n",
    "    text = \" \".join(sentences)\n",
    "    #----------------------------------------------------------------------#\n",
    "    \n",
    "    #----------------------------------------------------------------------#\n",
    "    # removing stopwords \n",
    "    #default_stopwords = stopwords.words('english')\n",
    "    #stop_words = default_stopwords\n",
    "    #tokens = [w for w in word_tokenize(text) if w not in stop_words]\n",
    "    #text = \" \".join(tokens)\n",
    "    #----------------------------------------------------------------------#\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we cover a very important subject: **clinical note truncation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_document(note):\n",
    "    '''\n",
    "    - truncates each note around \"social history\" section\n",
    "    - 600 and 900 characters\n",
    "    - to help combat BERT's sequence length limitation\n",
    "    - this code set the groundwork for the other\n",
    "      truncation functions (alcohol version shown below)\n",
    "    - we ultimately truncate the notes to much shorter \n",
    "      character sequences\n",
    "    '''\n",
    "    \n",
    "    before, _, after = note.partition(\"social history\")\n",
    "    before = before[-600:]\n",
    "    after = after[:900] \n",
    "    \n",
    "    note = before + _ + after\n",
    "    return note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind the following cell is to find a minimum set of sentimentally valuable keywords found in all positive (alcohol, in this example) samples. \n",
    "- I came up with these keywords after extensive analysis of the clinical notes. \n",
    "- Yes, this is the minimum amount. No other variation works, and if \"etoh\" was removed, for example, the positive samples only containing \"etoh\" would not be truncated (& *counter1 == counter2* would yield *False*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter1=0\n",
    "counter2=0\n",
    "for i, label in enumerate(binary_labels): \n",
    "    if label == 1:\n",
    "        counter1+=1\n",
    "        if (\"alco\" in dataset[i]) or(\"drink\" in dataset[i])or(\"etoh\" in dataset[i])or \\\n",
    "           (\"wine\" in dataset[i])or(\"ethanol\" in dataset[i])  or(\"beer\" in dataset[i])or \\\n",
    "           (\"drik\" in dataset[i]): # mispelled outlier\n",
    "            counter2+=1\n",
    "        else: \n",
    "            print(filenames.index(filenames[i]))\n",
    "            print(filenames[i])\n",
    "            print(raw_labels[i])\n",
    "            print(dataset[i], \"\\n\")\n",
    "counter1 == counter2 # want True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_dataset_alcohol(dataset, labels, b_num, a_num):\n",
    "    '''\n",
    "    - after identifying set of keywords, truncate note around\n",
    "      first-detected keyword to some specified amount of \n",
    "      characters before and after\n",
    "    - this procedure was repeated for tobacco and drug\n",
    "    '''\n",
    "    \n",
    "    # character length of two random notes before truncation\n",
    "    rand_num1 = random.randint(0, len(dataset))\n",
    "    rand_num2 = random.randint(0, len(dataset))\n",
    "    print(len(dataset[rand_num1]), len(dataset[rand_num2]))\n",
    "    #------------------------------------------\n",
    "    \n",
    "    #------------------------------------------\n",
    "    for index, document in enumerate(dataset):\n",
    "\n",
    "        # most specific first\n",
    "        if \"drik\" in document: \n",
    "            keyword = \"drik\"\n",
    "        elif \"wine\" in document:\n",
    "            keyword = \"wine\"\n",
    "        elif \"beer\" in document:\n",
    "            keyword = \"beer\"\n",
    "        elif \"ethanol\" in document:\n",
    "            keyword = \"ethanol\"\n",
    "        elif \"etoh\" in document:\n",
    "            keyword = \"etoh\"\n",
    "        elif \"drink\" in document:\n",
    "            keyword = \"drink\"\n",
    "        elif \"alco\" in document:\n",
    "            keyword = \"alco\"\n",
    "        else:\n",
    "            keyword = \"social history\" \n",
    "        # fallback keyword, explicitely mentioned in every note\n",
    "    \n",
    "        before, _, after = document.partition(keyword)\n",
    "        before = before[-b_num:] \n",
    "        after = after[:a_num] \n",
    "\n",
    "        document = before + _ + after\n",
    "        dataset[index] = document   \n",
    "    #------------------------------------------\n",
    "    \n",
    "    #------------------------------------------\n",
    "    dataset_truncated = dataset.copy()\n",
    "    # character length of two random notes after truncation\n",
    "    print(len(dataset[rand_num1]), len(dataset[rand_num2]))\n",
    "    return dataset_truncated\n",
    "\n",
    "# 35 before, 165 after - as described in the abstract\n",
    "dataset_truncated = truncate_dataset_alcohol(dataset, binary_labels, 35, 165) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the abstract, truncating notes to 35 characters before (& 165 after) the first detected keyword (for tobacco, alcohol, and drug) kept each clinical note at just under 64 tokens while simultaneously preserving the important sentiment within each clinical note. This token length was necessary to combat BERT's sequence length limitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
